# skelet-annotator

A GUI application for annotating **skeletal keypoints** on images and extracted video frames.  
Supports custom skeletons, **auto-annotation** via Ultralytics YOLO, and export to the **YOLO keypoints format** for training.

<div align="center">
  <img src="docs/screenshot_main.png" width="600"/>
</div>

<div align="center">
  <img src="docs/screenshot_points.png" width="420"/>
</div>

---

## ğŸš€ Features

- Load an **image folder** or a **video** (frames are extracted automatically).
- Skeleton setup: import from JSON or define _keypoints_ and _connections_ manually.
- Convenient labeling: add/drag keypoints, live table of coordinates, keyboard shortcuts.
- **Auto-annotation** using an Ultralytics YOLO model directly from the GUI.
- **Interpolation** of keypoints between annotated frames.
- Export annotations to:
  - **JSON** (keypoints + bbox computed from user keypoints),
  - **YOLO** (bbox from the model + user keypoints, Ultralytics keypoints format).

---

## ğŸ“¦ Installation

### Option A. Conda + pip (recommended)

Create and activate an environment:
```bash
conda create -n cv python=3.12 -y
conda activate cv
```

Install core conda deps and all project pip packages (including PySide6 and PyTorch):
```bash
conda install -y numpy pandas scipy matplotlib ipykernel

# GUI + computer vision + utilities
pip install pyside6 shiboken6 ultralytics ultralytics-thop opencv-python pyqtgraph pylsl tqdm sympy requests jinja2 pillow colorama psutil py-cpuinfo PyYAML typing-extensions

# PyTorch (choose the wheel appropriate for your system/driver)
# CUDA example:
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126
# or CPU-only:
# pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
```

### Option B. Using environment files

If the repo contains `environment.yml` and/or `requirements.txt`:
```bash
conda env create -f environment.yml
conda activate cv
# or
pip install -r requirements.txt
```

> **Windows tip:** Install **PySide6 via pip** (not conda) to avoid Qt DLL conflicts.

---

## ğŸ“‚ Data Preparation

You can either select an **image directory** or a **video file**â€”the app will offer to extract frames.

Example structure:
```
data/
â”œâ”€â”€ image_0001.jpg
â”œâ”€â”€ image_0002.jpg
â”œâ”€â”€ ...
â””â”€â”€ video_1.mp4
```

For auto-annotation, load an Ultralytics YOLO **keypoints** model (`.pt`). See:  
https://docs.ultralytics.com/tasks/keypoints/

---

## â–¶ï¸ Getting Started

Run the GUI:
```bash
python labelboxV3.py
```

Typical workflow:
1. **Choose data**: an image folder or a video to extract frames.
2. **Configure the skeleton**: **File â†’ Setup Skeleton** (manual entry or import from JSON).
3. **(Optional) Load a YOLO model**: left toolbar â€œopenâ€ button â†’ select `.pt` (Ultralytics).
4. Annotate points by clicking; drag to adjust positions.
5. Use **â€œInterpolateâ€** to fill in keypoints on in-between frames.
6. Save annotations: **File â†’ Save Annotations Asâ€¦** (JSON).
7. Export to **YOLO**: **File â†’ Save in YOLO format** (splits into `train/val`).

YOLO export structure:
```
dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â””â”€â”€ val/
â””â”€â”€ labels/
    â”œâ”€â”€ train/
    â””â”€â”€ val/
```
Each `.txt` line includes a bbox (from the model) and keypoints (from user annotations) in Ultralytics format.

---

## ğŸ§© Example Skeleton JSON

```json
{
  "keypoints": [
    "Nose", "LEye", "REye", "LEar", "REar",
    "LShoulder", "RShoulder", "LElbow", "RElbow",
    "LWrist", "RWrist", "LHip", "RHip", "LKnee", "RKnee",
    "LAnkle", "RAnkle"
  ],
  "connections": [
    ["Nose", "LEye"], ["Nose", "REye"],
    ["LEye", "LEar"], ["REye", "REar"],
    ["LShoulder", "RShoulder"],
    ["LShoulder", "LElbow"], ["LElbow", "LWrist"],
    ["RShoulder", "RElbow"], ["RElbow", "RWrist"],
    ["LShoulder", "LHip"], ["RShoulder", "RHip"],
    ["LHip", "RHip"],
    ["LHip", "LKnee"], ["LKnee", "LAnkle"],
    ["RHip", "RKnee"], ["RKnee", "RAnkle"]
  ]
}
```
Load it via **File â†’ Setup Skeleton â†’ Load from JSON**.

---

## ğŸ’¾ Save Formats

- **JSON** â€” stores `keypoints`, `connections`, and per-image coordinates. A `bbox` is also computed from user keypoints.
- **YOLO** â€” uses **bbox from the model** (Ultralytics) and adds **your keypoints** in Ultralytics keypoints format (normalized coordinates + confidence). Files are automatically split into `train/val` sets.

---

## â›‘ï¸ Troubleshooting (Windows)

If you see:
```
qt.qpa.plugin: Could not load the Qt platform plugin "windows" ...
```
this is typically a Qt plugin path conflict. Fixes:

1) Ensure PySide6/shiboken6 are installed **via pip** in the active env.
```powershell
pip install --force-reinstall PySide6 shiboken6
python -c "import PySide6, shiboken6; print(PySide6.__version__)"
```

2) Reset conflicting env vars and point to the platform plugins path:
```powershell
$env:QT_DEBUG_PLUGINS="1"
Remove-Item Env:QT_PLUGIN_PATH -ErrorAction SilentlyContinue
$env:QT_QPA_PLATFORM_PLUGIN_PATH = "$((python -c 'import pathlib,PySide6; print((pathlib.Path(PySide6.__file__).parent / \"plugins\" / \"platforms\").as_posix())'))"
python labelboxV3.py
```

3) Do not mix conda Qt with pip Qt in the same environment.

---

## ğŸ“œ License

MIT (or specify your own license).

---

## ğŸ™Œ Acknowledgements

- [Ultralytics YOLO](https://github.com/ultralytics/ultralytics)
- Qt for Python (PySide6)
